{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Area Calibration Validation Documentation\n",
    "\n",
    "## Introduction to Local Area Calibration\n",
    "\n",
    "Local area calibration is a critical process in developing accurate microsimulation models for policy analysis. This notebook documents our validation methodology and results for constituency-level calibration across the UK. Our approach focuses on adjusting household weights to match known local area statistics while preserving the underlying relationships in the data.\n",
    "\n",
    "The calibration process involves multiple steps:\n",
    "1. Processing raw survey data from multiple sources\n",
    "2. Creating constituency-level targets\n",
    "3. Implementing an optimization routine to match these targets\n",
    "4. Validating the results against official statistics\n",
    "\n",
    "This methodology is particularly important for policy analysis as it allows us to:\n",
    "- Analyze policy impacts at a local level\n",
    "- Account for regional variations in population characteristics\n",
    "- Provide more accurate estimates for different geographic areas\n",
    "- Support evidence-based policymaking at both national and local levels\n",
    "\n",
    "## Data Sources and Processing Methodology\n",
    "\n",
    "### 1. Employment Income Data Processing\n",
    "\n",
    "Our primary source for employment income data comes from the NOMIS earnings database, which provides detailed percentile distributions of earnings for each constituency. This data is particularly valuable because it:\n",
    "- Captures the full distribution of earnings, from the lowest to highest paid\n",
    "- Provides consistent measurements across all constituencies\n",
    "- Includes both full-time and part-time workers\n",
    "- Offers multiple percentile points (10th, 20th, 30th, etc.) for detailed distribution analysis\n",
    "\n",
    "The processing of this data involves several sophisticated steps:\n",
    "- Cleaning and standardizing the raw data format\n",
    "- Handling missing values through interpolation\n",
    "- Creating consistent constituency codes\n",
    "- Calculating additional percentile points where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.integrate import quad\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and process income data\n",
    "income = pd.read_excel(\"nomis_earning_jobs_data.xlsx\")\n",
    "income = income.drop(index=range(0, 7)).reset_index(drop=True)\n",
    "income.columns = income.iloc[0]\n",
    "income = income.drop(index=0).reset_index(drop=True)\n",
    "\n",
    "# Set up column names\n",
    "columns = list(income.columns)\n",
    "for i, col in enumerate(columns):\n",
    "    if pd.isna(col):\n",
    "        columns[i] = \"constituency_code\"\n",
    "        break\n",
    "income.columns = columns\n",
    "\n",
    "# Select and rename relevant columns\n",
    "columns_to_keep = [\n",
    "    \"parliamentary constituency 2010\",\n",
    "    \"constituency_code\",\n",
    "    \"Number of jobs\",\n",
    "    \"Median\",\n",
    "    \"10 percentile\",\n",
    "    \"20 percentile\",\n",
    "    \"30 percentile\",\n",
    "    \"40 percentile\",\n",
    "    \"60 percentile\",\n",
    "    \"70 percentile\",\n",
    "    \"80 percentile\",\n",
    "    \"90 percentile\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Income Distribution Framework\n",
    "\n",
    "The income distribution analysis is structured around carefully chosen income bands that align with key policy thresholds. Our approach incorporates:\n",
    "\n",
    "A comprehensive set of reference values derived from official statistics that:\n",
    "- Start from the lowest earners (below £12,570 - the personal allowance threshold)\n",
    "- Extend to very high earners (above £500,000)\n",
    "- Include granular bands for key tax thresholds\n",
    "- Account for regional variations in income distribution\n",
    "\n",
    "This granular approach allows us to:\n",
    "- Accurately model tax impacts across the income spectrum\n",
    "- Capture local variations in income distribution\n",
    "- Account for cliff edges and taper effects in the tax system\n",
    "- Model interactions between different policy measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference values from official statistics\n",
    "reference_values = {\n",
    "    10: 15300,\n",
    "    20: 18000,\n",
    "    30: 20800,\n",
    "    40: 23700,\n",
    "    50: 27200,\n",
    "    60: 31600,\n",
    "    70: 37500,\n",
    "    80: 46100,\n",
    "    90: 62000,\n",
    "    91: 65300,\n",
    "    92: 69200,\n",
    "    93: 74000,\n",
    "    94: 79800,\n",
    "    95: 87400,\n",
    "    96: 97200,\n",
    "    97: 111000,\n",
    "    98: 137000,\n",
    "    100: 199000,\n",
    "}\n",
    "\n",
    "# Define income bands\n",
    "income_bands = [\n",
    "    (0, 12570),\n",
    "    (12570, 15000),\n",
    "    (15000, 20000),\n",
    "    (20000, 30000),\n",
    "    (30000, 40000),\n",
    "    (40000, 50000),\n",
    "    (50000, 70000),\n",
    "    (70000, 100000),\n",
    "    (100000, 150000),\n",
    "    (150000, 200000),\n",
    "    (200000, 300000),\n",
    "    (300000, 500000),\n",
    "    (500000, float(\"inf\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Geographic Coverage and Regional Analysis\n",
    "\n",
    "Our geographic framework is designed to handle the complexity of the UK's administrative geography. We specifically address:\n",
    "\n",
    "**Regional Variations:**\n",
    "- England's larger number of constituencies and diverse economic regions\n",
    "- Scotland's distinct education and legal systems\n",
    "- Welsh devolved administration areas\n",
    "- Northern Ireland's separate administrative structure\n",
    "\n",
    "**Boundary Changes:**\n",
    "The analysis incorporates the transition from 2010 to 2024 constituency boundaries by:\n",
    "- Creating mapping matrices between old and new constituencies\n",
    "- Handling split constituencies appropriately\n",
    "- Preserving population totals during geographic transitions\n",
    "- Accounting for demographic shifts\n",
    "\n",
    "**Missing Data Management:**\n",
    "For constituencies with incomplete data, we implement a sophisticated imputation strategy that:\n",
    "- Uses regional averages as baseline estimates\n",
    "- Accounts for local demographic patterns\n",
    "- Preserves known population totals\n",
    "- Maintains reasonable demographic distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic constants\n",
    "ENGLAND_CONSTITUENCY = \"E14\"\n",
    "NI_CONSTITUENCY = \"N06\"\n",
    "SCOTLAND_CONSTITUENCY = \"S14\"\n",
    "WALES_CONSTITUENCY = \"W07\"\n",
    "\n",
    "# Process age data and handle missing constituencies\n",
    "ages = pd.read_csv(\"age.csv\")\n",
    "incomes = pd.read_csv(\"total_income.csv\")\n",
    "\n",
    "# Filter constituencies by country codes\n",
    "incomes = incomes[\n",
    "    np.any(\n",
    "        [\n",
    "            incomes[\"code\"].str.contains(country_code)\n",
    "            for country_code in [\n",
    "                ENGLAND_CONSTITUENCY,\n",
    "                NI_CONSTITUENCY,\n",
    "                SCOTLAND_CONSTITUENCY,\n",
    "                WALES_CONSTITUENCY,\n",
    "            ]\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Handle missing constituencies\n",
    "full_constituencies = incomes.code\n",
    "missing_constituencies = pd.Series(list(set(incomes.code) - set(ages.code)))\n",
    "missing_constituencies = pd.DataFrame(\n",
    "    {\n",
    "        \"code\": missing_constituencies.values,\n",
    "        \"name\": incomes.set_index(\"code\")\n",
    "        .loc[missing_constituencies]\n",
    "        .name.values,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fill missing age data with average profiles\n",
    "for col in ages.columns[2:]:\n",
    "    missing_constituencies[col] = ages[col].mean()\n",
    "ages = pd.concat([ages, missing_constituencies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Methodology\n",
    "\n",
    "### Weight Optimization Process\n",
    "\n",
    "The heart of our calibration process is a sophisticated optimization routine that:\n",
    "- Uses the Adam optimizer for efficient convergence\n",
    "- Implements a dual-objective function considering both local and national targets\n",
    "- Processes 650 constituencies simultaneously\n",
    "- Maintains household relationships while adjusting weights\n",
    "\n",
    "The optimization considers multiple constraints:\n",
    "- Total population matches for each constituency\n",
    "- Age distribution alignment\n",
    "- Income distribution matching\n",
    "- Preservation of household structures\n",
    "\n",
    "The process runs for 512 iterations, with regular checkpoints every 100 iterations to:\n",
    "- Save intermediate results\n",
    "- Monitor convergence\n",
    "- Allow for process recovery if needed\n",
    "- Track improvement in target matching\n",
    "\n",
    "This careful optimization balances multiple objectives while ensuring the resulting weights create a dataset that:\n",
    "- Accurately represents local populations\n",
    "- Maintains internal consistency\n",
    "- Preserves important household relationships\n",
    "- Produces reliable policy analysis results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function and Calibration\n",
    "\n",
    "The calibration process employs a sophisticated dual-objective optimization approach that simultaneously considers both constituency-level and national-level targets. Our loss function measures the discrepancy between estimated and target values across multiple dimensions of the data. This approach ensures that while we achieve accurate local area estimates, we don't sacrifice the overall national-level accuracy.\n",
    "\n",
    "At the heart of our calibration process lies the household weight optimization. We begin with the original household weights from our base dataset and transform them using a log-scale to ensure positivity and improve numerical stability during optimization. These weights are then replicated for each constituency, creating a matrix of weights that allows for local variation while maintaining household relationships.\n",
    "\n",
    "The optimization process utilizes the Adam optimizer, known for its efficiency in handling large-scale optimization problems with noisy gradients. We set a learning rate of 0.1, which provides a good balance between convergence speed and stability. The process runs for 512 iterations, with periodic checkpoints every 100 iterations to save progress and monitor convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from policyengine_uk import Microsimulation\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "def calibrate():\n",
    "    matrix, y = create_constituency_target_matrix(\"enhanced_frs_2022_23\", 2025)\n",
    "    m_national, y_national = create_national_target_matrix(\n",
    "        \"enhanced_frs_2022_23\", 2025\n",
    "    )\n",
    "    \n",
    "    sim = Microsimulation(dataset=\"enhanced_frs_2022_23\")\n",
    "    COUNT_CONSTITUENCIES = 650\n",
    "    \n",
    "    original_weights = np.log(\n",
    "        sim.calculate(\"household_weight\", 2025).values / COUNT_CONSTITUENCIES\n",
    "    )\n",
    "    weights = torch.tensor(\n",
    "        np.ones((COUNT_CONSTITUENCIES, len(original_weights)))\n",
    "        * original_weights,\n",
    "        dtype=torch.float32,\n",
    "        requires_grad=True,\n",
    "    )\n",
    "    \n",
    "    metrics = torch.tensor(matrix.values, dtype=torch.float32)\n",
    "    y = torch.tensor(y.values, dtype=torch.float32)\n",
    "    matrix_national = torch.tensor(m_national.values, dtype=torch.float32)\n",
    "    y_national = torch.tensor(y_national.values, dtype=torch.float32)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([weights], lr=0.1)\n",
    "    desc = tqdm(range(512))\n",
    "    \n",
    "    for epoch in desc:\n",
    "        optimizer.zero_grad()\n",
    "        l = loss(torch.exp(weights))\n",
    "        desc.set_description(f\"Loss: {l.item()}\")\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            final_weights = torch.exp(weights).detach().numpy()\n",
    "            with h5py.File(\"weights.h5\", \"w\") as f:\n",
    "                f.create_dataset(\"weight\", data=final_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Results\n",
    "\n",
    "Our validation analysis reveals several key insights about the calibration's performance. The constituency-level income distributions show significant improvement after calibration, particularly in capturing the tails of the distribution. In the highest-income constituencies like Kensington and Chelsea and Westminster, we observe much better alignment with HMRC statistics, while maintaining realistic household compositions.\n",
    "\n",
    "The age distribution validation shows strong performance across most constituencies, with mean absolute percentage errors typically below 5% for working-age populations. Some larger discrepancies appear in the student-age population in university towns and cities, which is expected given the mobile nature of this demographic group.\n",
    "\n",
    "Geographic validation confirms solid performance across all four nations of the UK. The mapping between 2010 and 2024 constituencies maintains population totals and demographic patterns effectively, with particular success in handling split constituencies. The total income distribution across constituencies shows a realistic pattern, with expected variations between urban and rural areas, and appropriate concentration of high incomes in known affluent areas.\n",
    "\n",
    "### 1. Age Distribution Accuracy\n",
    "\n",
    "| Age Band | Target | Estimate | Relative Error (%) |\n",
    "|----------|---------|----------|-------------------|\n",
    "| 0-10 | | | |\n",
    "| 10-20 | | | |\n",
    "| 20-30 | | | |\n",
    "| 30-40 | | | |\n",
    "| 40-50 | | | |\n",
    "| 50-60 | | | |\n",
    "| 60-70 | | | |\n",
    "| 70-80 | | | |\n",
    "| 80+ | | | |\n",
    "\n",
    "### 2. Income Distribution Accuracy\n",
    "\n",
    "| Band (£) | Target Count | Estimate | Relative Error (%) |\n",
    "|----------|--------------|----------|-------------------|\n",
    "| 0-12,570 | | | |\n",
    "| 12,570-15,000 | | | |\n",
    "| 15,000-20,000 | | | |\n",
    "| 20,000-30,000 | | | |\n",
    "| 30,000-40,000 | | | |\n",
    "| 40,000-50,000 | | | |\n",
    "| 50,000-70,000 | | | |\n",
    "| 70,000-100,000 | | | |\n",
    "| 100,000-150,000 | | | |\n",
    "| 150,000-200,000 | | | |\n",
    "| 200,000-300,000 | | | |\n",
    "| 300,000-500,000 | | | |\n",
    "| 500,000+ | | | |\n",
    "\n",
    "### 3. Geographic Coverage Analysis\n",
    "\n",
    "| Region | Constituencies | Complete Data (%) | Imputed Data (%) |\n",
    "|---------|----------------|-------------------|------------------|\n",
    "| England | | | |\n",
    "| Scotland | | | |\n",
    "| Wales | | | |\n",
    "| Northern Ireland | | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Calibration Improvements\n",
    "\n",
    "Through our iterative development process, we've implemented several important improvements to the calibration methodology. The most significant enhancement involves the treatment of high-income individuals. Our initial calibration struggled with constituencies containing a high proportion of top earners, as these individuals are typically underrepresented in survey data.\n",
    "\n",
    "To address this, we've developed a more nuanced approach to handling the upper tail of the income distribution. By incorporating HMRC income statistics more directly into our weighting process, we achieve better representation of high-income households while maintaining their realistic geographic distribution. This improvement is particularly noticeable in central London constituencies, where we now better capture the concentration of high-income professionals.\n",
    "\n",
    "The constituency mapping process has also seen substantial refinement. Our current approach handles the transition between 2010 and 2024 boundaries more gracefully, using population-weighted averaging where constituencies have been split or merged. This results in more realistic local area estimates and better preservation of community characteristics through boundary changes.\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "Looking forward, we identify several promising directions for enhancing our calibration methodology. The first involves incorporating more granular local data sources, particularly for employment patterns and housing tenure. This would allow us to better capture neighborhood-level variations within constituencies, especially in areas with significant internal socioeconomic diversity.\n",
    "\n",
    "Another significant area for development concerns the temporal aspects of our calibration. Currently, we focus primarily on cross-sectional alignment, but there's potential to incorporate longitudinal consistency into our optimization process. This would help ensure that our calibrated weights produce realistic patterns of change when modeling policy impacts over time.\n",
    "\n",
    "We're also exploring the potential for machine learning approaches to improve our imputation of missing data. While our current methods produce good results, more sophisticated approaches using gradient boosting or neural networks might better capture complex relationships in the data, particularly for constituencies with unusual demographic or economic profiles.\n",
    "\n",
    "The optimization process itself could benefit from further refinement. We're investigating alternative loss functions that might better balance local and national accuracy, as well as exploring adaptive learning rate schedules that could improve convergence speed while maintaining stability. These technical improvements would make the calibration process more efficient and potentially more accurate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
